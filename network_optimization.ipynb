{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from  deap import base, creator, tools, algorithms\n",
    "import joblib\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,accuracy_score\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening data\n",
    "\n",
    "data = pd.read_csv(\"all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_spins</th>\n",
       "      <th>total_wins</th>\n",
       "      <th>volitily</th>\n",
       "      <th>total_payout</th>\n",
       "      <th>avg_payout</th>\n",
       "      <th>total_bonus_spins</th>\n",
       "      <th>bonus_spin_prec</th>\n",
       "      <th>freq_Orange</th>\n",
       "      <th>freq_Apple</th>\n",
       "      <th>freq_Grape</th>\n",
       "      <th>...</th>\n",
       "      <th>multi_Orange</th>\n",
       "      <th>multi_Apple</th>\n",
       "      <th>multi_Grape</th>\n",
       "      <th>multi_Strawberry</th>\n",
       "      <th>multi_Mango</th>\n",
       "      <th>multi_Banana</th>\n",
       "      <th>multi_Cherry</th>\n",
       "      <th>multi_Lemon</th>\n",
       "      <th>multi_Seven</th>\n",
       "      <th>multi_Bonus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1500</td>\n",
       "      <td>1870</td>\n",
       "      <td>71.266667</td>\n",
       "      <td>1596.89</td>\n",
       "      <td>0.853952</td>\n",
       "      <td>396</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>0.136973</td>\n",
       "      <td>0.034764</td>\n",
       "      <td>0.084669</td>\n",
       "      <td>...</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.53</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1500</td>\n",
       "      <td>2738</td>\n",
       "      <td>84.400000</td>\n",
       "      <td>4981.67</td>\n",
       "      <td>1.819456</td>\n",
       "      <td>859</td>\n",
       "      <td>57.266667</td>\n",
       "      <td>0.014936</td>\n",
       "      <td>0.024051</td>\n",
       "      <td>0.169189</td>\n",
       "      <td>...</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.42</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1500</td>\n",
       "      <td>1577</td>\n",
       "      <td>64.866667</td>\n",
       "      <td>2395.09</td>\n",
       "      <td>1.518763</td>\n",
       "      <td>393</td>\n",
       "      <td>26.200000</td>\n",
       "      <td>0.004715</td>\n",
       "      <td>0.042726</td>\n",
       "      <td>0.100152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.65</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500</td>\n",
       "      <td>2085</td>\n",
       "      <td>74.333333</td>\n",
       "      <td>2501.84</td>\n",
       "      <td>1.199923</td>\n",
       "      <td>145</td>\n",
       "      <td>9.666667</td>\n",
       "      <td>0.144910</td>\n",
       "      <td>0.093950</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>...</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.36</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1500</td>\n",
       "      <td>2722</td>\n",
       "      <td>81.733333</td>\n",
       "      <td>2475.21</td>\n",
       "      <td>0.909335</td>\n",
       "      <td>176</td>\n",
       "      <td>11.733333</td>\n",
       "      <td>0.137766</td>\n",
       "      <td>0.089967</td>\n",
       "      <td>0.168387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_spins  total_wins   volitily  total_payout  avg_payout  \\\n",
       "0         1500        1870  71.266667       1596.89    0.853952   \n",
       "1         1500        2738  84.400000       4981.67    1.819456   \n",
       "2         1500        1577  64.866667       2395.09    1.518763   \n",
       "3         1500        2085  74.333333       2501.84    1.199923   \n",
       "4         1500        2722  81.733333       2475.21    0.909335   \n",
       "\n",
       "   total_bonus_spins  bonus_spin_prec  freq_Orange  freq_Apple  freq_Grape  \\\n",
       "0                396        26.400000     0.136973    0.034764    0.084669   \n",
       "1                859        57.266667     0.014936    0.024051    0.169189   \n",
       "2                393        26.200000     0.004715    0.042726    0.100152   \n",
       "3                145         9.666667     0.144910    0.093950    0.010810   \n",
       "4                176        11.733333     0.137766    0.089967    0.168387   \n",
       "\n",
       "   ...  multi_Orange  multi_Apple  multi_Grape  multi_Strawberry  multi_Mango  \\\n",
       "0  ...          1.12         1.65         0.53              0.98         2.00   \n",
       "1  ...          1.75         1.08         1.41              1.97         1.08   \n",
       "2  ...          0.45         0.50         0.71              1.35         1.56   \n",
       "3  ...          1.31         0.71         1.96              1.01         1.29   \n",
       "4  ...          0.48         0.43         0.89              1.34         1.60   \n",
       "\n",
       "   multi_Banana  multi_Cherry  multi_Lemon  multi_Seven  multi_Bonus  \n",
       "0          0.56          0.23         0.53           11          NaN  \n",
       "1          1.70          0.88         1.42           10          NaN  \n",
       "2          1.70          1.81         0.65           13          NaN  \n",
       "3          1.13          0.48         1.36            7          NaN  \n",
       "4          1.46          0.30         0.86            9          NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# calculating RTP\n",
    "data['RTP'] = data['total_payout'] / data['total_spins'] * 100\n",
    "\n",
    "# droping useless columns\n",
    "data = data.drop(columns=['multi_Bonus','total_spins','total_wins','total_payout'])\n",
    "\n",
    "# features, target\n",
    "features = data.drop(columns=['volitily','RTP','total_bonus_spins','bonus_spin_prec','avg_payout'])\n",
    "target  = data[['RTP', 'volitily', 'total_bonus_spins', 'bonus_spin_prec', 'avg_payout']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq_Orange</th>\n",
       "      <th>freq_Apple</th>\n",
       "      <th>freq_Grape</th>\n",
       "      <th>freq_Strawberry</th>\n",
       "      <th>freq_Mango</th>\n",
       "      <th>freq_Banana</th>\n",
       "      <th>freq_Cherry</th>\n",
       "      <th>freq_Lemon</th>\n",
       "      <th>freq_Seven</th>\n",
       "      <th>freq_Bonus</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_Wild</th>\n",
       "      <th>multi_Orange</th>\n",
       "      <th>multi_Apple</th>\n",
       "      <th>multi_Grape</th>\n",
       "      <th>multi_Strawberry</th>\n",
       "      <th>multi_Mango</th>\n",
       "      <th>multi_Banana</th>\n",
       "      <th>multi_Cherry</th>\n",
       "      <th>multi_Lemon</th>\n",
       "      <th>multi_Seven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.136973</td>\n",
       "      <td>0.034764</td>\n",
       "      <td>0.084669</td>\n",
       "      <td>0.117806</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0.079536</td>\n",
       "      <td>0.069167</td>\n",
       "      <td>0.140603</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.103775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053418</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.53</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014936</td>\n",
       "      <td>0.024051</td>\n",
       "      <td>0.169189</td>\n",
       "      <td>0.046295</td>\n",
       "      <td>0.060868</td>\n",
       "      <td>0.186584</td>\n",
       "      <td>0.124470</td>\n",
       "      <td>0.152583</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.129607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070314</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.42</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004715</td>\n",
       "      <td>0.042726</td>\n",
       "      <td>0.100152</td>\n",
       "      <td>0.119236</td>\n",
       "      <td>0.109668</td>\n",
       "      <td>0.082998</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>0.121818</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.112285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101544</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.65</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.144910</td>\n",
       "      <td>0.093950</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>0.123265</td>\n",
       "      <td>0.055281</td>\n",
       "      <td>0.094311</td>\n",
       "      <td>0.106850</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>0.043256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156194</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.36</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.137766</td>\n",
       "      <td>0.089967</td>\n",
       "      <td>0.168387</td>\n",
       "      <td>0.161699</td>\n",
       "      <td>0.011938</td>\n",
       "      <td>0.043555</td>\n",
       "      <td>0.012796</td>\n",
       "      <td>0.117493</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.034187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080296</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   freq_Orange  freq_Apple  freq_Grape  freq_Strawberry  freq_Mango  \\\n",
       "0     0.136973    0.034764    0.084669         0.117806    0.016789   \n",
       "1     0.014936    0.024051    0.169189         0.046295    0.060868   \n",
       "2     0.004715    0.042726    0.100152         0.119236    0.109668   \n",
       "3     0.144910    0.093950    0.010810         0.123265    0.055281   \n",
       "4     0.137766    0.089967    0.168387         0.161699    0.011938   \n",
       "\n",
       "   freq_Banana  freq_Cherry  freq_Lemon  freq_Seven  freq_Bonus  ...  \\\n",
       "0     0.079536     0.069167    0.140603    0.003911    0.103775  ...   \n",
       "1     0.186584     0.124470    0.152583    0.001459    0.129607  ...   \n",
       "2     0.082998     0.119629    0.121818    0.002311    0.112285  ...   \n",
       "3     0.094311     0.106850    0.012048    0.003274    0.043256  ...   \n",
       "4     0.043555     0.012796    0.117493    0.000213    0.034187  ...   \n",
       "\n",
       "   freq_Wild  multi_Orange  multi_Apple  multi_Grape  multi_Strawberry  \\\n",
       "0   0.053418          1.12         1.65         0.53              0.98   \n",
       "1   0.070314          1.75         1.08         1.41              1.97   \n",
       "2   0.101544          0.45         0.50         0.71              1.35   \n",
       "3   0.156194          1.31         0.71         1.96              1.01   \n",
       "4   0.080296          0.48         0.43         0.89              1.34   \n",
       "\n",
       "   multi_Mango  multi_Banana  multi_Cherry  multi_Lemon  multi_Seven  \n",
       "0         2.00          0.56          0.23         0.53           11  \n",
       "1         1.08          1.70          0.88         1.42           10  \n",
       "2         1.56          1.70          1.81         0.65           13  \n",
       "3         1.29          1.13          0.48         1.36            7  \n",
       "4         1.60          1.46          0.30         0.86            9  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RTP</th>\n",
       "      <th>volitily</th>\n",
       "      <th>total_bonus_spins</th>\n",
       "      <th>bonus_spin_prec</th>\n",
       "      <th>avg_payout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106.459333</td>\n",
       "      <td>71.266667</td>\n",
       "      <td>396</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>0.853952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332.111333</td>\n",
       "      <td>84.400000</td>\n",
       "      <td>859</td>\n",
       "      <td>57.266667</td>\n",
       "      <td>1.819456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>159.672667</td>\n",
       "      <td>64.866667</td>\n",
       "      <td>393</td>\n",
       "      <td>26.200000</td>\n",
       "      <td>1.518763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>166.789333</td>\n",
       "      <td>74.333333</td>\n",
       "      <td>145</td>\n",
       "      <td>9.666667</td>\n",
       "      <td>1.199923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165.014000</td>\n",
       "      <td>81.733333</td>\n",
       "      <td>176</td>\n",
       "      <td>11.733333</td>\n",
       "      <td>0.909335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RTP   volitily  total_bonus_spins  bonus_spin_prec  avg_payout\n",
       "0  106.459333  71.266667                396        26.400000    0.853952\n",
       "1  332.111333  84.400000                859        57.266667    1.819456\n",
       "2  159.672667  64.866667                393        26.200000    1.518763\n",
       "3  166.789333  74.333333                145         9.666667    1.199923\n",
       "4  165.014000  81.733333                176        11.733333    0.909335"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_holdout, y_holdout, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaleing \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape[1]\n",
    "y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 2s 3ms/step - loss: 64153.0625\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 14055.9229\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 7913.0518\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 5217.3887\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 4458.2368\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 4229.1226\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 4017.0930\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 3834.7896\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 3665.3679\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 3486.0210\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 3333.0215\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 3181.1362\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 3065.3464\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 2936.4487\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 2828.2695\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 2712.8728\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 2630.5820\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 2525.9958\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 2463.6582\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 2388.2820\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 2305.7910\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 2251.4387\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 2195.1287\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 2141.1313\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 2089.6003\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 2037.4714\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1991.8287\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1967.2887\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1907.0869\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1875.5184\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1843.9019\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1787.8422\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1772.0168\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1728.8071\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1696.6495\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1669.5397\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1630.4955\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1602.8354\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1575.1490\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1540.4238\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1515.8113\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1471.8198\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1464.9934\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1413.9381\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1402.8744\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1360.9098\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1351.7937\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1314.2753\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1297.2556\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1270.1487\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1243.5533\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1220.6890\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1214.6582\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1175.2124\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1177.5890\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1144.3914\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1125.2603\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1105.1392\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1093.2246\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1069.2510\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1060.4319\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1045.2355\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 1029.0781\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1010.7895\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1005.3585\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 987.8627\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 972.6091\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 961.5351\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 953.9468\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 937.8062\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 921.0029\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 912.8873\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 894.5316\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 895.4899\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 876.0903\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 860.0268\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 856.6577\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 842.4607\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 832.1644\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 823.0518\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 808.4403\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 800.0875\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 786.3624\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 788.4316\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 772.5580\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 759.6254\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 756.8685\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 744.9241\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 744.1052\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 731.1008\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 724.9722\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 715.2160\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 706.2304\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 694.0350\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 694.6881\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 691.2432\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 680.4404\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 669.4121\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 668.8800\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 662.8526\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 930.7830\n",
      "Loss: 930.782958984375\n",
      "44/44 [==============================] - 0s 1ms/step\n",
      "Mean Absolute Error: 13.826475896865594\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGwCAYAAABPSaTdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABcHUlEQVR4nO3deXhU5dk/8O9kmywkQxaSmUDEgKEShyWAbKIgJBjZtPSVTay2lLJLBEqg1hdwSYS3AvoiWClKlQKtr1DBhRJEkbAIJgQJ8YcIYZNMEQiTQFZmzu+POEMmmeWc5Mz+/VxXrsucOXPyzEnLuXM/z30/CkEQBBARERH5qAB3D4CIiIjImRjsEBERkU9jsENEREQ+jcEOERER+TQGO0REROTTGOwQERGRT2OwQ0RERD4tyN0D8ARGoxGXL19GZGQkFAqFu4dDREREIgiCgMrKSiQmJiIgwHb+hsEOgMuXLyMpKcndwyAiIqIWuHjxIjp06GDzdQY7ACIjIwE03KyoqCg3j4aIiIjEqKioQFJSkvk5bguDHcA8dRUVFcVgh4iIyMs4WoLCBcpERETk0xjsEBERkU9jsENEREQ+jcEOERER+TQGO0REROTTGOwQERGRT2OwQ0RERD6NwQ4RERH5NAY7RERE5NPYQZmIiIgsGIwCjpRex5XKGsRHhqJvcgwCA7x3o2wGO0RERGS2q7gMy3aWoExfYz6mUYViyehUZGo1bhxZy3Eai4iIiAA0BDozNhVaBDoAoNPXYMamQuwqLnPTyFqHwQ4RERHBYBSwbGcJBCuvmY4t21kCg9HaGZ6NwQ4RERHhSOn1ZhmdxgQAZfoaHCm97rpByYTBDhEREeFKpe1ApyXneRIGO0RERIT4yFBZz/MkDHaIiIgIfZNjoFGFwlaBuQINVVl9k2NcOSxZMNghIiIiBAYosGR0KgA0C3hM3y8ZneqV/XYY7BAREREAIFOrwbrJvaBWWU5VqVWhWDe5l9f22WFTQSIiIjLL1GqQkapmB2UiIiLyXYEBCgzoHOvuYciG01hERETk0xjsEBERkU/jNBYRERE5hafsns5gh4iIiGTnSbuncxqLiIiIZOVpu6cz2CEiIiLZeOLu6Qx2iIiISDaeuHs6gx0iIiKSjSfuns5gh4iIiGTjibunsxqLiMiNPKU0l0gupt3Tdfoaq+t2FGjYa8uVu6cz2CEichNPKs0lkotp9/QZmwqhACwCHnftnu72aawff/wRkydPRmxsLMLDw9GzZ08UFBSYXxcEAUuXLkViYiLCwsIwZMgQnDx50uIatbW1mDNnDuLi4hAREYExY8bg0qVLrv4oRESieVppLpGcPG33dLdmdsrLy/HAAw/g4YcfxmeffYb4+HicOXMGbdu2NZ+zYsUKrFy5Ehs3bkSXLl3w8ssvIyMjA6dOnUJkZCQAICsrCzt37sTWrVsRGxuL+fPnY9SoUSgoKEBgYKCbPh0RkXWOSnMVaCjNzUhVc0qLvJYn7Z6uEATBdYXuTSxatAgHDhzA/v37rb4uCAISExORlZWF7OxsAA1ZnISEBCxfvhzTpk2DXq9Hu3bt8P7772P8+PEAgMuXLyMpKQmffvopHnnkEYfjqKiogEqlgl6vR1RUlHwfkIjIikNnrmHi+sMOz9sytb9P7TxNJDexz2+3TmPt2LEDffr0wRNPPIH4+HikpaVh/fr15tdLS0uh0+kwfPhw8zGlUonBgwfj4MGDAICCggLU19dbnJOYmAitVms+p6na2lpUVFRYfBERuYonluYS+TK3Bjtnz57FunXrkJKSgn//+9+YPn06nn32Wbz33nsAAJ1OBwBISEiweF9CQoL5NZ1Oh5CQEERHR9s8p6nc3FyoVCrzV1JSktwfjYjIJk8szSXyZW4NdoxGI3r16oWcnBykpaVh2rRpmDp1KtatW2dxnkJhOb8nCEKzY03ZO2fx4sXQ6/Xmr4sXL7bugxARSWAqzbX1r5gCDVVZrizNJfJlbg12NBoNUlNTLY517doVFy5cAACo1WoAaJahuXLlijnbo1arUVdXh/LycpvnNKVUKhEVFWXxRUTkKqbSXADNAh53leYS+TK3BjsPPPAATp06ZXHs+++/R8eOHQEAycnJUKvVyMvLM79eV1eHffv2YeDAgQCA3r17Izg42OKcsrIyFBcXm88hIvI0nlaaS+TL3Fp6/txzz2HgwIHIycnBuHHjcOTIEbz99tt4++23ATRMX2VlZSEnJwcpKSlISUlBTk4OwsPDMWnSJACASqXClClTMH/+fMTGxiImJgYLFixAt27dkJ6e7s6PR0RklyeV5hL5MrcGO/fffz+2b9+OxYsX48UXX0RycjJWr16NJ5980nzOwoULUV1djZkzZ6K8vBz9+vXD7t27zT12AGDVqlUICgrCuHHjUF1djWHDhmHjxo3ssUNEHi8wQMHyciInc2ufHU/BPjtERETexyv67BARERE5G4MdIiIi8mkMdoiIiMinMdghIiIin8Zgh4iIiHwagx0iIiLyaQx2iIiIyKe5takgERH5HoNRYFdo8igMdoiISDa7isuwbGcJyvQ15mMaVSiWjE7lfl/kNpzGIiIiWewqLsOMTYUWgQ4A6PQ1mLGpELuKy9w0MvJ3DHaIiKjVDEYBy3aWwNr+Q6Zjy3aWwGD0+x2KyA0Y7BARUasdKb3eLKPTmACgTF+DI6XXXTcoop8x2CEiola7Umk70GnJeURyYrBDREStFh8ZKut5RHJisENERK3WNzkGGlUobBWYK9BQldU3OcaVwyICwGCHiIhkEBigwJLRqQDQLOAxfb9kdCr77ZBbMNghIiJZZGo1WDe5F9Qqy6kqtSoU6yb3Yp8dchs2FSQikoDdge3L1GqQkarmPSKPwmCHiEgkdgcWJzBAgQGdY909DCIzTmMREYnA7sBE3ovBDhGRA+wOTOTdGOwQETnA7sBE3o3BDhGRA+wOTOTdGOwQETnA7sBE3o3VWETkVL5Qqm3qDqzT11hdt6NAQy8Zdgcm8kwMdojIaXylVNvUHXjGpkIoAIuAh92BiTwfp7GIyCl8rVSb3YGJvBczO0QkO0el2go0lGpnpKq9KhvC7sBE3onBDhHJTkqptrd12mV3YCLvw2ksIpIdS7WJyJMw2CEi2bFUm4g8CYMdIpKdqVTb1koWBRqqsliqTUSuwGCHiGRnKtUG0CzgYak2Ebkagx0icgqWahORp2A1FhE5DUu1icgTMNghIqdiqTYRuRunsYiIiMinMdghIiIin8Zgh4iIiHwa1+wQEZHbGYwCF7KT0zDYISIit9pVXIZlO0ss9lPTqEKxZHQqWxSQLDiNRUREbrOruAwzNhU22zhWp6/BjE2F2FVc5qaRkS9xa7CzdOlSKBQKiy+1Wm1+XRAELF26FImJiQgLC8OQIUNw8uRJi2vU1tZizpw5iIuLQ0REBMaMGYNLly65+qMQkZcyGAUcOnMNHxX9iENnrsFgFNw9JL9hMApYtrME1u646diynSX8nVCruX0a67777sOePXvM3wcGBpr/e8WKFVi5ciU2btyILl264OWXX0ZGRgZOnTqFyMhIAEBWVhZ27tyJrVu3IjY2FvPnz8eoUaNQUFBgcS0ioqY4feJeR0qvN8voNCYAKNPX4EjpdfZqolZx+zRWUFAQ1Gq1+atdu3YAGrI6q1evxvPPP4+xY8dCq9Xib3/7G6qqqrB582YAgF6vx4YNG/Daa68hPT0daWlp2LRpE06cOGERQBERNcXpE/e7Umk70GnJeUS2uD3YOX36NBITE5GcnIwJEybg7NmzAIDS0lLodDoMHz7cfK5SqcTgwYNx8OBBAEBBQQHq6+stzklMTIRWqzWfY01tbS0qKiosvojIf3D6xDPER4Y6PknCeUS2uDXY6devH9577z38+9//xvr166HT6TBw4EBcu3YNOp0OAJCQkGDxnoSEBPNrOp0OISEhiI6OtnmONbm5uVCpVOavpKQkmT8ZEXkyKdMn5Dx9k2OgUYXCVoG5Ag3Tin2TY1w5LPJBbg12Hn30UfzqV79Ct27dkJ6ejk8++QQA8Le//c18jkJh+X8DQRCaHWvK0TmLFy+GXq83f128eLEVn4KIvA2nTzxDYIACS0anAkCzgMf0/ZLRqey3Q63m9mmsxiIiItCtWzecPn3aXJXVNENz5coVc7ZHrVajrq4O5eXlNs+xRqlUIioqyuKLiPwHp088R6ZWg3WTe0GtsrzXalUo1k3uxYXiJAu3V2M1Vltbi++++w4PPvggkpOToVarkZeXh7S0NABAXV0d9u3bh+XLlwMAevfujeDgYOTl5WHcuHEAgLKyMhQXF2PFihVu+xxE5NlM0yc6fY3VdTsKNDxsm06fsMuvc2RqNchIVfPektO4NdhZsGABRo8ejbvuugtXrlzByy+/jIqKCjz99NNQKBTIyspCTk4OUlJSkJKSgpycHISHh2PSpEkAAJVKhSlTpmD+/PmIjY1FTEwMFixYYJ4WIyKyxjR9MmNTIRSARcBja/qEZerOFRigYHk5OY1bg51Lly5h4sSJuHr1Ktq1a4f+/fvj8OHD6NixIwBg4cKFqK6uxsyZM1FeXo5+/fph9+7d5h47ALBq1SoEBQVh3LhxqK6uxrBhw7Bx40b22CEiu0zTJ00DGLWVAMZUpt40C2QqU+d0C5FnUwiC4Pe1lRUVFVCpVNDr9Vy/Q+QCnjQd5GgsBqOAQcv32qzeMk155WcP9dhpF0+630RyEvv89qg1O0Tk+zxtOsjR9Im3d/n1tPtN5A4eVY1FRO7jij2ivLFrsTeXqXvj/SZyBmZ2iMglf/2L6Vq8aNsJRCqD0b9zrMdMs3hrmbqj+61AQ5fojFS1x9xrImdhZofIz7nqr39H00EAcKOqHk9u+BqDlu/1mKyDt3b5ZZdoojsY7BD5MVfuESVlmqesFYGW3NNx3trl15un34jkxmksIj/WmsW3Uit8pE7zCJA+zeKs6TgpZeqewlun34icgcEOkR9r6V//LQkqHHUttkZKlZOze+F4W5fflnaJJvJFnMYikpkrqprk0pK//lu6xqfxdJAUYgIyMdNxz28vxvbCS636nZjK1B/r2R4DPGgRtTXeOv1G5AzM7BDJyNt6mkj967+1FT6m6aBFH57Ajep6UWMUE5CJmY67dqsOz/3zOADP/p2IJWYa0Run34icgcEOkUy8cUsBqXtEydFgL1OrQWRoMJ7869cOxxcTESxqmkXqItvGvxNvmpoykRJUe9v0G5EzMNghkoE39zSR8te/XBU+/TvFQqMKdViK/lT/u0X9vJYsflagoa/P0h0l0FV4RyYOaFlQzU02yd8x2CGSgbdvKSD2r/+WrPGxNd1iyijZWz3z+uen8c9vLjoMPlqy+FlAQ18fwHI6zZMzcd4cVBO5E4MdIhn4Qk8TMX/9S13j42i6xVpGqSkxwYe96TipPDlo8PagmshdWI1FJAN/6WkipcJHTNVWplaD/Oyh+Pvv+qFtWLDVnym2uaEpeFKrWn+PPbW7sC8E1UTuwGCHSAbeuqVAS9gKKtSqUHP2RUpn5sAABQIUCrvVWWKDD1PwtGVqf6wa3xMxEdYDKLE8LWjwl6CaSG6cxiKSgdSqJm/naI2P1OkWsUHFZz/38LFXTdR4Oi4sOAAzNhWaf6ZUnhY0sFEgUcsws0MkEzEZD19ir8Ge1OkWsUHFe4fOY+L6w6I3Cm3N1JYnZuLYKJCoZZjZIZIRe5o0kDrdIrWaSkrFVOPfyWfFZXjv0HlRY2tJ0CB1v7CWYKNAIukY7BDJjD1NxAUvbcPvNAxsPA0ohtSKqca/EzHBznPpXSQHDa7sns2gmkgaTmMR+SB3789lCl7s/dQbVfXIK9GZv8/UavD7h5Ih9nndkoopRwvJAUAdpcTsofeIvibQ8v3CWsOb9ukicjdmdoh8jDMzDFKmaTJS1WgbHvxz477mmmZmdhWX4e2vSiUvJJZSMSVmIfnSMfdJChzY6I/I8zGzQ+RDnJlh2FVchkHL92Li+sOYu7XI4ULhI6XXbQY6gGVmxl7A4IjUiim5F5JLqTwjIvdgZofIRzgzw9CS/ZikVGQ5ChisaU2ZtZxrXtjoj8jzMdgh8hHO2kqgpUGUlIosqYGAHGXWci0kZ6M/Is/HaSwiH+GsDENLp2mkdJWWGgh4Uu8if+qeTeStGOwQ+QhnZRhaGkRJaYAnpkoqJiIYq8b3xJap/ZGfPdQjAh2Ajf6IvAGDHSIf4awMQ2uCKLGLgR0FDAoAOb/shl+meWaZtb91zybyNgpBEFzbgMMDVVRUQKVSQa/XIyoqyt3DIWox00JiwHpZtb0Hr62ycoNRwKDlex3ux5SfPdRmECK2ZN2VjfmcwRUdlInoDrHPbwY7YLBDvqUlAYOj97QmiJKKAQMRicVgRwIGO+RrpAQMtsrKmwYy3p51ISLfw2BHAgY75K9MU1S2qq2aTlEx60JEnkTs85t9dsjv+doDXMrnkdqbx1pvGl+7f0TkexjskF/79Nsy/OmjYly/VWc+5s1TM1KnmsSWleeV6Kw24JN7aouBExE5A6exwGksf5X7aQn+8lWp1dcUkHfRrSuIXXtTd9uI9w+dw/nrVRAEAe8fviDq+m81uR9if56U8XNNEBFJwTU7EjDY8T+ffnsZMzcfs3uOxkE5tTWOMhPOylyIXXszqrsGG/JLYZT4//qma3fqbhvRP3cPrt+yvaO5o3L0xqQGTswAERHANTtENhmMAv70UbHD86TuIyWmfNtZmQuxa2/W77eeyXKk8dodfXUd/ri92Gag0/R8R/dP6t5bu4rLsHRHCXQVdz6vOioUS8cwA0RE1jHYIb9zpPS63Qd1Y2LXtDjaFfz3DyXj7a9KJe0aLoYpw/FZcZnk97ZEXokO7x44ZzUwsUbM/ZOySFpfXYfpP/f7aUxXUYPpmwqbTbV5C2aqiJyLwQ75HSkbYYrZKkFMZmL9/uaBTuPXre0a7oi1DIez/avosuhABxB3/8T+PnT6aiz7uMTuOYu3nZB8H5tydeDBtUpEzsdgh/yO2L2eYiNCRO0jJSYzYW9lnJQpH5NdxWVWMxzO1EYZaFG1Zo9pzU7f5BiHwYPY38fVm3W4UWU/I1deVY/DZ6/hgXviRF2zKVcHHo4ygt62SJ7IUzHYIb9j2jDTXoACAC89phX1F72UTJEc1zEYBSzadkL0dRWApGyMLTdrDZLOXzI6FXklOofBg+n34WjvrRvV4gKtQ2daFuy4OvCQulaJiFpO0q7ngiDg9OnTKCkpwe3bt501JiKnMu2wbe/xMe2hZIzoLu7BJjYzIdd1Dp+95jDD0VhClBJrJvSEo+dlgAKICg0UfV1bYiKCsW5yLwDAjE2FzYJKU/Dw6beXcejMNXz87WVMuD/J/IBvzPS9o9+XJemhnaPAA2gIPAxSy9jskLJWiYhaR3Swc+7cOfTs2RP33nsvunXrhnvuuQeFha5NoxPJJVOrwbrJvaBRWQYYMRHBWDspDYtHpIq+likzYethrADsBhoKNGQ7xEyZAQ2ZC2kUCAoKwNQHk+2eNaKbBhU10rI3TUWFBuFA9jBkpKrtBg8CgNlbjmHi+sOYu7UIq/acRtvwYKjCgy3OVatCzRmVAZ3EZWvEnteYOwIPsZk8uTKHRP5MdLCTnZ2NmpoavP/++/jggw+g0Wgwffp02QaSm5sLhUKBrKws8zFBELB06VIkJiYiLCwMQ4YMwcmTJy3eV1tbizlz5iAuLg4REREYM2YMLl26JNu4yHdlajXIzx6KLVP74/UJPbFlan8cfT4DI7onSrqOKVME2M5MTH0wGQo7ry8ZnSphqkJaduE/FQ2ZlLS7ojHtoeRmgVeAoiGTlZGaIOm61lTU3MbQ177Emr2nHU4TNk2S6Kvqoa+qx3PpXcy/j/zsoeapo/6dY9G2STDUVNvwYPQXue6pMXcEHmIzeXJlDon8mehgZ//+/Xj77bcxadIkjB07Fh988AEKCgpQXV3d6kEcPXoUb7/9Nrp3725xfMWKFVi5ciXWrFmDo0ePQq1WIyMjA5WVleZzsrKysH37dmzduhX5+fm4efMmRo0aBYOhdX+hkn8w7fX0WM/25r2fWsKUKVI3yRSZMhOLR6TafV3KWhCpmYvG0zALM7vi/730KF4Y2RW/HtARL4xs+H7xiFTZHqo6fQ1W7Tkt+X2mcW49egGjuic2+30EBijw6thudq/x6thuLfoduiPwEJMRlJLxIyLbRHdQDggIQFlZGRIS7vz116ZNGxQXF+Puu+9u8QBu3ryJXr16Ye3atXj55ZfRs2dPrF69GoIgIDExEVlZWcjOzgbQkMVJSEjA8uXLMW3aNOj1erRr1w7vv/8+xo8fDwC4fPkykpKS8Omnn+KRRx4RNQZ2UCa5uKKDssEooPfLeZLW7ZhsmdrfZsWXqQuzrYXCrmRvnA0l9yehq6g1H1NHKbF0zH0tXkDs6LNL7QgtlmlRNGCZr2vplhtE/kbs81t0ZkehUCAgwPL0gIAAtHa3iVmzZmHkyJFIT0+3OF5aWgqdTofhw4ebjymVSgwePBgHDx4EABQUFKC+vt7inMTERGi1WvM51tTW1qKiosLii0gOjjJFcmSSxGQ4bLE3DWNvOs7V7I0zU6vBgUXDLKYfDywa1qqgQMxUpLSpRnEcZQQZ6BDJQ3TpuSAI6NKlCxSKO/9nv3nzJtLS0iyCoOvXxS/g27p1KwoLC3H06NFmr+l0OgCwyCSZvj9//rz5nJCQEERHRzc7x/R+a3Jzc7Fs2TLR4yRyB3tZoEytBm9N7tUsw+GIo2mYTK0Gb05K+3kneOmZI2taUvruaJymoFFOpsCjaam82skN/jK1GmSkqtlBmciJRAc77777rqw/+OLFi5g7dy52796N0FDb/7A1Dq6AhqCr6bGmHJ2zePFizJs3z/x9RUUFkpKSRI6cyPnENLdr/JDU6avx0iffofxWnd1pGEfrP3YVl+GlT76TLdB5Lr0Lth69YPE5AhTNFydLHaezuCvwcEbwRkR3iA52kpOTMXDgQAQFydOHsKCgAFeuXEHv3r3NxwwGA7766iusWbMGp06dAtCQvdFo7vxFdeXKFXO2R61Wo66uDuXl5RbZnStXrmDgwIE2f7ZSqYRSqZTlcxCJIWWtjqPmdm9O6oXoiJBm1woLCcSMTYXNMimm78f3ScKqvO8BCBjQKQ79m0yj2fq5LWEKWmYPvQezh95j8dnLb9Vh1mbb61ScMV0kBQMPIt8jeoFyYGAgysrKEB8fL8sPrqysNE9HmfzmN7/Bvffei+zsbNx3331ITEzEc889h4ULFwIA6urqEB8f32yB8qZNmzBu3DgAQFlZGTp06MAFyuQxpGxBYFooa69su2lmRKMKxQsjuyI6Qok9JTpsL/rRIjPTNjwY9beNuFVnWaHYNjwYr47thkytRtTPFUvM4lruB0VEchD7/Ja0ZkdOkZGR0Gq1FsciIiIQGxtrPp6VlYWcnBykpKQgJSUFOTk5CA8Px6RJkwAAKpUKU6ZMwfz58xEbG4uYmBgsWLAA3bp1a7bgmdzLX3d1lroFgaPmdkDzKaAyfQ1mbj5mcSwmIgSP90yEKiwEq/Z8b/U6N6rqzTuFq8JCZAl0AHFrXLhOhZzFX/+tIfskzUk5Wisjt4ULF6K6uhozZ85EeXk5+vXrh927dyMyMtJ8zqpVqxAUFIRx48ahuroaw4YNw8aNGxEY2Pq29yQPf/0rviV7H8nVtK78Vh3ePXAOqjD7TfgAYOmOk8h+tKvknxETEYxf9myPoV0TAAG4equ22cPF3oOH00UkN3/9t4Yck9Rn5/e//z3Cw8Ptnrdy5UpZBuZKnMZyHluZDX/oI3LozDVMXH/Y4XmNe8qIfY/cXhjZFS998p2o8+IilaL+YuaDh1zJn/+t8WeyT2MBwIkTJxASEmLzdVdnfsiz+eOuzo0zGaf/c1PUexpncxztAO4sl8qrHJ6jUYXimQeSRf2uXL2DOPk3f/y3hqSRFOxs375dtgXK5PukbK4o53SGu+bsrWUyxGjcUyYwQIEXRnZttgbH2T46XubwnBdGdhV1H/ngIVdz17815D1EBzvM2pBU7thc0V1TJy0p27bWU8bU58aVIkICcP1WncPzoiPEtWvgg4dcjTvIkyOyVmNVVVU5XNND/sPVmyu6a+rEXibDFtOfDi+M7GrOQp27WoXVe76XbfpKgYby8nIHe2jdqjOKup7cDxQ+eEgu3EGeHJHUQVmlUll9raamBm+++Sb+53/+x+42DeRfHK0/kbNbrrOmTsRMiYkpF29KrQrFmB4avPTJd7KVfDclAMgd2w1GIzBrSyFa2z1C7gdKXIQSh85cY4kwtZor/60h7yQ62Jk4cSKWLVuG3bt3Izg4GAsXLsTjjz+Od999F88//zwUCgXmzp3rzLGSlzFtrmirqy8gX7dcZ0ydiJ0SE5uheFSbgEytxqKLsDMXIbcNDzb3smltoNM2LBhGowDDz01+7AWAYh48qvBgzP/gOHQVrNSi1nPlvzXknUQHO0uXLsWbb76JjIwMHDhwAE888QR++9vf4ssvv0Rubi4mTZqE4GDHPT3Iv7hqc8WWTJ3Yy9pImRITm8n4rPg/eKxne/RNjsGg5XudXm11o6re/Plafa3qejy54Wu0DQ82X9ukaZDi6MEjmN9vOb3GSi1qDXdt5EreQXSw889//hMbN27EL3/5Sxw/fhxpaWmoqKjAyZMnZdsvi3yTK7rlSp1isZe1GXpvAv64/YToKTFTJsPRdJTpfZGhwU6bumrKdL/lcsPK+h9rQYqtB09ClBI1t41Wr8NKLWotduYmW0RHKRcvXsT9998PAOjRowdCQkKQnZ3NQIdEcXa3XClz9vayNtM3FaKNMgg3a2/b/FlNp8RMmYzpmwrtjtH0vkNnrkn8dC1n+sdeTDDWUraCFGsPHqMg4Mm/fm33WqzUotZgZ26yJkDsifX19RYNBYODg20uWCZyNVPAAdyZozdpPGcPwO5CZgB2A53GGk8PZWo1mPLA3SJH6/x2gQo0ZKpMf9WaPruzNA5SGjM9eB7r2R4DOsfi6s1aUddjpRYRyUlSWua///u/zaXldXV1ePnll5sFPN64XQT5hkytBm9OSsOfPiq22PW78Zz9oTPXZMtwNJ0eSk9VY8OBcw7fN6BTHD4s/NFpXZKtLcjM1GqwdlIaZm851mwjUTk5ClJYIkxE7iA62HnooYdw6tQp8/cDBw7E2bNnLc5h40FyJ1NDvsaBTkxECF4YmYqMVDUOnbmGz4oddwoWQ2OljFXsdJG+us7mAl6xwkMCMe2hTkiJj8RLn4hbkDmieyLWQIGZm+1Pt7WGoyCFJcJE5A6iNwL1ZdwI1PvZ2wRQQEMZtrVFsS21dlIaHtFqmi2E/HexzmEwoVGFIj97KPJKdC3aXsLkrZ8XBEvdHiP30xKs318qe4anbVgw3nyyF/p3inW4QeiMn9c3WSsRZjUWEYkl9vnNYAcMdrydwShg0PK9LqtwAoCsYffgvcPnLbJIGlUoJtx/F1bt+d7h+/8+pR8eSIkzByoHfriKNV/8IGkMbZRBKHwhAyFBopfeOdzWYlQ3DT45Udaq6TUx/XK4IzoRyYHBjgQMdjybo8zFoTPXMHH9YTeOULrwkECsHNfD/GB/cedJvCNivU9T0eHByB3bzXwde/fKUVBomkJ6YWRXm52drfXZsXYdwHGGxl0bthKR7xD7/GbdOHk0MRkAb6zcqaozmHvTAGhRoAMA5VX1Ftexd6/EdpkuKavAn/+rB6AArlTW4vrNWsREhECtCjOvpTl85hpmbS7EjeqW98thiTARuQozO2Bmx1PZW4cD3MkceGNmx0SjCoUgCNBViCvJtsbehp+N71XtbSPmbi2SNDZb00pi7/mWqf0Z0BCR04h9fouf7CdyIUcbewINmQODUTBX+HjjBEiZvqZVgQ7QcD9s7Wze+F7FtVFKuq6pM/IuKxVs3NmciLyJqGmsb7/9VvQFu3fv3uLBEJlI3diztaXcvsx0ryDAbtm3tffZmo5ivxwi8iaigp2ePXtCoVBAEASHvXQMBoMsAyP/JjVzYGsvJrrj6q1ayUGhre0b2C+HiLyJqGms0tJSnD17FqWlpfjwww+RnJyMtWvX4tixYzh27BjWrl2Lzp0748MPP3T2eMlPtCRzkKnVID97KLZM7Y/XJ/TE36f0gzrKszMLGlUoon+ucHK2+MhQc1CYIPG+NA0+xW7PweoqIvIEojI7HTt2NP/3E088gTfeeAMjRowwH+vevTuSkpLwwgsv4PHHH5d9kOR/xGQOEqKUMAoCPir60aJ0uW9yjLmkeWLfJKzac9pjp7eq6w2yNju0pmmWJVOrQaQyGE9usL0hZ1PWgk9b2TRbHZyJiNxFcun5iRMnkJyc3Ox4cnIySkpKZBkUkSlzYG3KxfR9zW2jxQ7aGlUoxvTQYMfxMouHr5jeMO7iikAHACbcfxc+/vayOSi8ekv8ougABdC7Y7TV16ztbM5+OUTkaSSXnvfq1Qtdu3bFhg0bEBra8NdebW0tfvvb3+K7775DYaHz9t1xFpaeey5rfXakbv1geuyO7K5B/umrVnvDuJqrMk3R4cEQYBlUSen0bMISciLyRE7roHzkyBGMHj0aRqMRPXr0AAAcP34cCoUCH3/8Mfr27du6kbsBgx3P1rjT7tmfbuJ/9/7g1J27vVlMeDBen5CG61V1OHf1FlbtOd3sHFPwp5IQNL4+oSce69lexpESEbWe0/rs9O3bF6WlpXjllVfQvXt3dOvWDTk5OSgtLfXKQIc8n6nTrjIoAK9/7tpAp40yCOEhzf9vEhrsmS2qrlfVY+GH3yI4QIGtRy9aPcd0+6RMNLGEnIi8GTsog5kdb+COzT69lZQpsrnD7rGbKTMtbs7PHsp1OETkcZzaQfn999/HoEGDkJiYiPPnzwMAVq1ahY8++qhloyVywFGTQU/noD2VrKT89dKpXRusmdjL6mueVEJuMAo4dOYaPir6EYfOXIOB85hEJIHkYGfdunWYN28eHn30UZSXl5ubCEZHR2P16tVyj48IgPdvO+Cp+dP4yFCM6K7BW5N7QaOynKpSq0Id7lzuCruKyzBo+V5MXH8Yc7cWYeL6wxi0fK/VbSyIiKyRPI2VmpqKnJwcPP7444iMjMTx48fRqVMnFBcXY8iQIbh69aqzxuo0nMbyfK3Z7NNTe+y4k7XpqcYLwT2lhFzsZrBE5J+cNo1VWlqKtLS0ZseVSiVu3bol9XJEDhmMAoyCgLZh9jsNByiAqQ8mW81QTH2weW8ouSiDPHOxsonYDsemheCP9WyPAZ1j3R7oSNkMlojIHslNBZOTk1FUVGTRVRkAPvvsM6Smpso2MCLAep8dW9ZMTMOI7olY9GjXZhmKI6XXsX5/qVPGWHvb6JTrtoYpc/PCyFS89Il3djiWuhksEZEtkoOdP/zhD5g1axZqamogCAKOHDmCLVu2IDc3F3/961+dMUbyU7amMJrSNHl4mzIUjTlzzY8CDQuQPS3BYLonj2jvdDiOa6MEhIZNQQ+dueYRU1W2SN0MlojIFsnBzm9+8xvcvn0bCxcuRFVVFSZNmoT27dvj9ddfx4QJE5wxRvJD9qYwTNqGB+PNib3Q38GUi8Eo4Gql+O0RpBLgWQuQ2yiD8OcnujcL/nYVl2HBB8ctsiVNA0VP0pLNYImIrJEc7ADA1KlTMXXqVFy9ehVGoxHx8fFyj4v8TNPFsUZBcDh1daOqHgEBCrsLbPNKdKKnwVwppV0EfvjpllMWTrdRBiIjVW1xzFaWTKevwYxNhR650FfMZrCNNzglIrJFcrAzdOhQbNu2DW3btkVcXJz5eEVFBR5//HHs3btX1gGS77O6/5WDxcgmpikMa9dQhQVBX33b4TVMOaER3dT45IRO/MBbSKEATv/kvMX8uoparMr7Hg/cE2cOBOwt9FX8/HpGqtqjprQcbQYLeEYPICLyfJKDnS+//BJ1dXXNjtfU1GD//v2yDIr8h62Mg9jNOuMjQ21eQ0ygAwDRESF4vGciokKDATg/2AkLDkRVncGpP2PNFz9gzRc/mDf99NaFvplaDdZN7tUskPWWRdZE5BlEBzvffvut+b9LSkqg0915KBgMBuzatQvt23OjQBJPzLocW0xTGL07RmPw/3zRqumg67fq8M6Bc624gjgBCmBENw0+/tZ1zfB0+hrRu5t76kLfTK0GGalqj+sBRETeQ3Sw07NnTygUCigUCgwdOrTZ62FhYfjf//1fWQdHvq2lW0A0nsIoOF/ucetxrMnoGo/M+9QovVbl0p8rJQj05IW+1irsiIjEEh3slJaWQhAEdOrUCUeOHEG7du3Mr4WEhCA+Ph6BgYFOGST5JrGZhLZhwRbTWo2nMD4q+tFZw5NV3ndXkPfdFVmvKVdnaC70JSJfJzrYMTURNBo9r4EaeSexmYRZD9+DVE0Urt6qbTaF4cnZCGdRAFCFByM0KBC6CmlZLS70JSJ/JLnPfW5uLt55551mx9955x0sX75clkGRfzCVFjt6xL7y6XeYtbkQ567earZWo29yDNqGi6vc8hUCGsruX3uiB7ZM7Y/ZD3cW9b7n0lOg9tDNPomInElysPOXv/wF9957b7Pj9913H9566y1J11q3bh26d++OqKgoREVFYcCAAfjss8/MrwuCgKVLlyIxMRFhYWEYMmQITp48aXGN2tpazJkzB3FxcYiIiMCYMWNw6dIlqR+LZGQwCjh05ho+KvoRh85cs7l3kam0GGi+f1NTN6rrsWrPafR+Oc9it+u8Eh1uVImr3PI1V2/VYkDnWDyX8Qu7QaMCDc0DZw9NQX72UGyZ2h+vT+iJLVP7Iz97KAMdIvJ5koMdnU4Hjab5P47t2rVDWZm0KpMOHTrg1VdfxTfffINvvvkGQ4cOxWOPPWYOaFasWIGVK1dizZo1OHr0KNRqNTIyMlBZWWm+RlZWFrZv346tW7ciPz8fN2/exKhRo2AwOLe0l6zbVVyGQcv3YuL6w5i7tQgT1x/GoOV7LQKUxkylxU0zDrbcqKrHjE2F2FVcZq7m8ldxEUoA9oPGptNUnrbZJxGRK0gOdpKSknDgwIFmxw8cOIDExERJ1xo9ejRGjBiBLl26oEuXLnjllVfQpk0bHD58GIIgYPXq1Xj++ecxduxYaLVa/O1vf0NVVRU2b94MANDr9diwYQNee+01pKenIy0tDZs2bcKJEyewZ88eqR+NWsnU76ZpdZSpS6+9gCc/eyj+q1cHUT9HAPD89mIcPH3VKyqxnGX+B8fN99RW0MhpKiKiFjQV/N3vfoesrCzU19ebS9A///xzLFy4EPPnz2/xQAwGAz744APcunULAwYMQGlpKXQ6HYYPH24+R6lUYvDgwTh48CCmTZuGgoIC1NfXW5yTmJgIrVaLgwcP4pFHHrH6s2pra1Fbe2evpIqKihaPmxrY65kjtkvvnu/+I/rnXbtVh6nvf9OisfoKXUUNpm8qxFs/BzPsR0NEZJ3kYGfhwoW4fv06Zs6cae6kHBoaiuzsbCxevFjyAE6cOIEBAwagpqYGbdq0wfbt25GamoqDBw8CABISEizOT0hIwPnz5wE0TKmFhIQgOjq62TmNmx42lZubi2XLlkkeK9nmqGeOoy69R0qvi+6abFJzm5WBALBo2wlzEMl+NEREzUmexlIoFFi+fDl++uknHD58GMePH8f169fx3//93y0awC9+8QsUFRXh8OHDmDFjBp5++mmUlNxZh6FQWP5VKghCs2NNOTpn8eLF0Ov15q+LFy+2aOx0h9ieObbO89Tuvd7gRlU91uz9wd3DICLyWJKDHZM2bdrg/vvvh1arhVKpbPEAQkJCcM8996BPnz7Izc1Fjx498Prrr0Otbti1uWmG5sqVK+Zsj1qtRl1dHcrLy22eY41SqTRXgJm+qHXE9ruxdZ4/9suR07sHS21WvRER+TtR01hjx47Fxo0bERUVhbFjx9o9d9u2ba0akCAIqK2tRXJyMtRqNfLy8pCWlgYAqKurw759+8z9fHr37o3g4GDk5eVh3LhxAICysjIUFxdjxYoVrRoHSWPqmaPT11hdt+OoS6+j95N9N6rqPXIjTyIiTyAq2FGpVOZpIZVKJdsP/+Mf/4hHH30USUlJqKysxNatW/Hll19i165dUCgUyMrKQk5ODlJSUpCSkoKcnByEh4dj0qRJ5rFMmTIF8+fPR2xsLGJiYrBgwQJ069YN6enpso2THDOVP8/YVNiiLr323u/PVGFBuG0QcEvELumf/VyZxUXJRESWFIIguO25MmXKFHz++ecoKyuDSqVC9+7dkZ2djYyMDAANWZ5ly5bhL3/5C8rLy9GvXz+8+eab0Gq15mvU1NTgD3/4AzZv3ozq6moMGzYMa9euRVJSkuhxVFRUQKVSQa/Xc0qrlXYVl2HZzhKLxcqaRntZteT9/koBYN3kXjilq8SqPadFv0/K/TYxGAVWcRGR1xH7/HZrsOMpGOzIq7UPTtP7dRU1WPh/x1Fv8L//icZEBCPnl92QqdXAYBTQ++U80Z2iTXdabH+d1gaoRETuImuwk5aW5rACyqSwsFD8KD0Egx3PZDAK6L50F27V+V+J+WtP9MCvet9psmhq2Cg27DOtkcrPHmo30LR1XakBExGRO4h9fotas/P444+b/7umpgZr165FamoqBgwYAAA4fPgwTp48iZkzZ7Zu1OQXbGV+mh43CoJfBjoAcKOqzuJ7U4dksVN8jvoaAfI0giQi8gaigp0lS5aY//t3v/sdnn32Wbz00kvNzmG/GnLE1pTJmB4a7DheZnE8qMWNEbxfdHhIs2ONOyR/VlyG9w6dd3gde/2LWtsIkojIW0juoPzBBx/gm2+at+mfPHky+vTpg3feeUeWgZHvsTVlUqavwV++Km12vj83SD52sRyBgQqbma+OMeGirmOvf1FrG0ESEXkLycFOWFgY8vPzkZKSYnE8Pz8foaFsDEfW2ZsyoebeP3wB7x++AMB25itAAdjqI+iorxHQ+kaQRETeQnKwk5WVhRkzZqCgoAD9+/cH0LBm55133mnxlhHk+xxNmZBttjJf9gIdwH5fI6D1jSCJiLyF5GBn0aJF6NSpE15//XVs3rwZANC1a1ds3LjR3MWYqClOhThP0wyPWmTZeGsbQRIReQv22QFLz13h0JlrmLj+sLuH4bN+2TMR7aPDMKBTHPp3jpUUoLDPDhF5K1lLz5u6ceMG/u///g9nz57FggULEBMTg8LCQiQkJKB9+/YtHjT5lsYLauPaKKGOUuI/FbVct+ME24suAwA+LPzRZpBiq+S/cZUXOygTkS+SnNn59ttvkZ6eDpVKhXPnzuHUqVPo1KkTXnjhBZw/fx7vvfees8bqNMzsyM9atqBteLDoLsDUMraaATJ7Q0S+SOzzW3Ink3nz5uGZZ57B6dOnLaqvHn30UXz11VctGy35FFOJedMFyfqfA53QYD9uoGND27AgqKNC0dpciukvl2U7S2D4eSGPrd+HTl+DGZsKsevnDUSJiHyV5KfO0aNHMW3atGbH27dvD51OJ8ugyHs56soLAKFBga4ckldQKBT471FdG/67lddq3AxQzO+jcWBEROSLJAc7oaGhqKioaHb81KlTaNeunSyDIu8lpsT8RjWnspoqr6pHdIQS6yb3glolT1+bK5U1krokExH5KsnBzmOPPYYXX3wR9fUNDyyFQoELFy5g0aJF+NWvfiX7AMm7sMS85a5U1iBTq0F+9lBsmdofsx/u3KrrxUeGsksyERFaEOz8+c9/xk8//YT4+HhUV1dj8ODBuOeeexAZGYlXXnnFGWMkL8Juuy1nuneBAQoM6ByL5zJ+AY1K+joeBRoWH/dNjmGXZCIitKD0PCoqCvn5+di7dy8KCwthNBrRq1cvpKenO2N85GVMXXnZLVmaAAVQfqvW4pi9pn+2NG0GyC7JREQSMzu3b99GUFAQiouLMXToUCxYsAALFy5koENmpgc03REbEYJpDyXbzdAYBWDW5mPNKqMytRqsm9wLCVGWmReNKhTTHkqGpsn6HrUq1KLsvPHvo+nPZ5dkIvIXkjI7QUFB6NixIwwGg7PGQx7OVmO6xjK1GjyX3gWr9nzvplF6jpiIYBxaPAwhQQHo0SEas7cU2tzTCmiojMpIVVsJPizfJAgC0u6KxsLMrqJ+H+sm92rWZ0fsthJERN5O8jTWn/70JyxevBibNm1CTAxT3/5ESmO6lPgIVw/PI+X8shtCghoSqNERIXYDncaVUQM6xwK40yOn6dv+U1GLGZsKmzUPtIVdkonIn0kOdt544w388MMPSExMRMeOHRERYflQKywslG1w5DlsPXRNjekaP3QNRgEvffKd6wfpYZ5L72IRiEitjHLUI0cBe5mg5kwLn4mI/I3kYOexxx6DQsG/Bv2J1IeumF47vi4hMgR97o7GR0U/mrMoUiujpPTIYRBDRGSb5GBn6dKlThgGeTKpD132bAFqDQKe/OvX5u81qlC8MLKrpMoo9sghIpKH6GqsqqoqzJo1C+3bt0d8fDwmTZqEq1evOnNs5CGkPnTPXa1y5nC8QtMNT3X6GszafAxjejRMa4mpjGKPHCIieYgOdpYsWYKNGzdi5MiRmDBhAvLy8jBjxgxnjo08hJSHrsEoYMuRC04ekeeytXTGlMnZcbwMb05qviVE05Jx4E7PIluTxo2bBxIRkW2ip7G2bduGDRs2YMKECQCAyZMn44EHHoDBYEBgIDd29GXlt+ocnmN66K7Zexq6Cv+dVhFTbRUdEYL87KEOK6PsNRVkjxwiIvFEZ3YuXryIBx980Px93759ERQUhMuXLztlYOQ6BqOAQ2eu4aOiH3HozDWLHbAbKqtKHF7jhZFdkVeiw6o9p505VI/RNLzQqELx2wfuFvXeK5U15sqox3q2x4DOsTYDFlOPHDGZICIisk50ZsdgMCAkJMTyzUFBuH37tuyDItdx1DtHbGWVKjwECz447syhepT/ndATsT9vtGnKzBwpvY53Dpxz+F6pa2zYI4eIqHVEBzuCIOCZZ56BUqk0H6upqcH06dMteu1s27ZN3hGS04jpnVN72yjqWofOXPObcvNpDyVjVM/2zY47cx8q9sghImo50cHO008/3ezY5MmTZR0MuY7Y3jl/fqKHyCuK2abSu8VGhOClx7QY0d361BHX2BAReSbRwc67777rzHGQRGL2qLJHbO8cCBCVrRjQKQ5rvjgj+XN4ullDOqOLOlL0PeY+VEREnkdyU0FyPyl7VNkipXeOmGzF/ckxUAYFiJ728haDUtpJnj7iGhsiIs8iuhqLPINpnU3TrIxpnc2u4jJR14mLUDo+CcCLHzdUYtmqCHpjYhr2lPwHPZb92+cCnbbhwTAaBYvqNLHEVlsREZHzKQRB8P3FFg5UVFRApVJBr9cjKirK3cOxyWAUMGj5XpvTT6YppfzsoQ4frgd+uGqxnYE9CgBvTuoFVVgwDp29CqMAtA0LxqfFl1F0sULip/A+UrNmRETkGmKf35zG8iJybgx59Wat6J8rAJi9pdBuwzxfZmtnd05TERF5BwY7XkTOjSGl9nrx10AHaL6ze16JrtVrpoiIyHW4ZseLyLkxpKN9l8iSKWu2Zu8PsqyZIiIi12Gw40Xk3BjS1BOGpHn3QKnN3kRAQ/anJQuaiYjIeRjseJHGAUrTgKclTevM+y5FSZvS8mc3quttvtZ4zRQREXkOBjteRu6NITO1GhxYNBTPpafIOUyfo0BDBZoYYtdWERGRa3CBshdyRtO6vsmx+O0D9fjnN5dws5abu1rzmwfuFrWru9TF30RE5FwMdryUXBtDWuvGTJYCFMCaiWl4RKvB1qMXnbLRJxEROQ+nsfyYrW7MZMkoANERStnXTBERkWu4NdjJzc3F/fffj8jISMTHx+Pxxx/HqVOnLM4RBAFLly5FYmIiwsLCMGTIEJw8edLinNraWsyZMwdxcXGIiIjAmDFjcOnSJVd+FK9jb9dzas60DkfuNVNEROR8bg129u3bh1mzZuHw4cPIy8vD7du3MXz4cNy6dct8zooVK7By5UqsWbMGR48ehVqtRkZGBiorK83nZGVlYfv27di6dSvy8/Nx8+ZNjBo1CgaDwR0fy+MZjAI2HihlRkeCxutwMrUa5GcPxZap/fH6hJ7YMrU/8rOHMtAhIvJQHrU31k8//YT4+Hjs27cPDz30EARBQGJiIrKyspCdnQ2gIYuTkJCA5cuXY9q0adDr9WjXrh3ef/99jB8/HgBw+fJlJCUl4dNPP8Ujjzzi8Od6y95YcuAaHek0Ivcbkxu3pCAiss8r98bS6/UAgJiYhgWepaWl0Ol0GD58uPkcpVKJwYMH4+DBg5g2bRoKCgpQX19vcU5iYiK0Wi0OHjxoNdipra1Fbe2dvaEqKnx/M0vgzhodj4luvYQ71uFYC0q5JQURUct4zAJlQRAwb948DBo0CFqtFgCg0+kAAAkJCRbnJiQkmF/T6XQICQlBdHS0zXOays3NhUqlMn8lJSXJ/XE8DtfoSBcdHoy3mmz+eejMNXxU9CMOnbnmtE7JthaOc0sKIqKW8ZjMzuzZs/Htt98iPz+/2WsKheVf1YIgNDvWlL1zFi9ejHnz5pm/r6io8PmAx9GO6dRgeGoCuiREYkDnWPTvFGvO6Lgq02IvKG26ISmntIiIxPGIzM6cOXOwY8cOfPHFF+jQoYP5uFqtBoBmGZorV66Ysz1qtRp1dXUoLy+3eU5TSqUSUVFRFl++zp+7+o7qJj4Y+c0DyVjwyC/wwD1xFoGOqzItjoJSbklBRCSdW4MdQRAwe/ZsbNu2DXv37kVycrLF68nJyVCr1cjLyzMfq6urw759+zBw4EAAQO/evREcHGxxTllZGYqLi83nkH939f3q9E+YMrCjw/OsbaLqKNMCyLv5p9ig1J+DVyIiqdw6jTVr1ixs3rwZH330ESIjI80ZHJVKhbCwMCgUCmRlZSEnJwcpKSlISUlBTk4OwsPDMWnSJPO5U6ZMwfz58xEbG4uYmBgsWLAA3bp1Q3p6ujs/nkfpmxyDtuHBuFFleyNLX1VRcxvvHDzv8LwXRjZfiCwl0yJHR2uxQak/B69ERFK5NdhZt24dAGDIkCEWx999910888wzAICFCxeiuroaM2fORHl5Ofr164fdu3cjMjLSfP6qVasQFBSEcePGobq6GsOGDcPGjRsRGBjoqo/isUzlyzp9NW4b/Hd5sphPHh0R0uyYqzMtfZNjoFGFcksKIiIZeVSfHXfx1T477KkjzesTeuKxnu0tjh06cw0T1x92+N4tU/vLktkB7qwRAiyDNFPOiZ2aiYgaiH1+e8QCZZIf972SztrUkCnTYqvuSQHra31ag1tSEBHJy2NKz0k+7Kkjjb2pIdPmnzM2FUIB65kWZzQdzNRqkJGqZgdlIiIZMNjxQeypI56YgMWUaWk6Jah2ckfjwACFbFNjRET+jMGOD2JZsm0BCqBxlbjYgIWZFiIi78Vgxwf5a1myRhWKMT00ePurUgDWp5zWTOyF6IiQFgUszLQQEXknBjs+yLSo1henspqumzF5Lj0Fs4emIDBAgbS7ol0+5URERJ6LpefwzdJzX9zhfFR3DQrOl4van8rUX4hTTkREvkvs85vBDnwz2AEaAp5F2074VNfktZNaPg1FRES+hcGOBL4a7AANGY43Pv8e/7v3B8i0fZNbqaOUOLBomNUAh9kcIiL/Ivb5zTU7Pi6vRIe/HTrvE4EOAOgqarFm7w+Ym55icdxat2hbU1xERORf2EHZRxiMAg6duYaPin7EoTPXYDAK+PTbMkzfVOhT01gAsGrP99hVXGb+3la3aJ2+BjM2FVqcS0RE/oeZHR9gLauhCgtCRfVtN47KuRZvO4GMVDUA2OwWLaChemvZzhJkpKo5pUVE5KcY7Hg5W1VXeh8OdACgvKoeh89eQ4BCYbfEXgBQpq/BkdLr7JFDROSnOI3lxfx9D6xDZ66J7hbNrtJERP6LmR0v0rTayGgUfLJxoHiC6G7R/tpVmoiIGOx4DWvrctooA904Ivcb0CnO3C1ap6+xmuGyt6M5ERH5B05jeQFb1UY3aw1uGpH7tQ0PRv/OsQgMUGDJ6FQAd/a/MhGzozkREfk+Bjsezt/X5djy6thu5gAmU6vBusm9oFZZTlWpVaFYN7kX++wQEfk5TmN5uCOl1/18XY6ltmFBePVX3ZsFMJlaDTJS1eygTEREzTDY8XD+XkX0q7REaNqGAVBgQOdY9O8UazOACQxQsLyciIiaYbDj4fy9iuihX8TjsZ7t3T0MIiLyYgx2PJyjaiNfJ1ewx01CiYj8F4MdD2eqNpqxqRAKwG8CHjlLxrlJKBGRf2M1lhewVW3k6+QoGecmoURExMyOl2hcbbSnRIftRT/i+i3f2s28MVV4cKuvYa9sn5uEEhH5D2Z2vIip2uiF0ffh6PMZGN1d7e4hOY2+qr7VmRdHZfuNNwklIiLfxWDHS3327WXs/Fbn7mE4jSkbs2xnCQzGlq1U4iahREQEMNjxOgajgNV532P21iJ3D8XpWpt54SahREQEcM2ORxBbFr2ruAyLtp3AjSrfXatjTUszL9wklIiIAAY7bie2LNpUVeQvpeeNtTTzYq9sn5uEEhH5D05juZHYsmh/3QxUgYbArzWZF24SSkREzOy4iZSyaH/cDFRM5kXs9B83CSUi8m8MdtxESlm0r1YLRYQE4u1f94G+qg4vffKdxf1QO+hwLLUrMjcJJSLyXwx23ERKWbSvVgu9Nq4HHrgnDgDwiFYjOvNia/2SafqP01NERNQYgx03kVIW3Tc5Bm3Dg32mCis6PBi/TGsPVVgIDEYBgQEK0ZkXdkUmIiKpuEDZTUxl0fYex40X59bdNrpmYE7WRhmI8qp6vHPgHCauP4xBy/dK6pLMrshERCQVgx03MZVFA7AZ8FTXG5BXosPhM9dQVWdw3eCcQBnU8D+1m7WWn0PqhpzsikxERFIx2HEjU1m0rU0vTftDbfr6nGsH5gThIYFWj0vdFoJdkYmISCoGO26WkapGaJD1X4Pp0b//9FXXDUhmCgAxEcEot7PeSMrUk6PpPzl68xARkW9hsONmR0qvQ1dRa/N1Ac2nfryFKSD5Zc/2os4XM/Vkb/qPXZGJiMgaBjtuJnZtidJG9seTmboUp6eqRZ0vduqJXZGJiEgKlp67mdgHfK2XVWPFRARj3x8eRkhQAAxGQfYNOdkVmYiIxHJruuCrr77C6NGjkZiYCIVCgX/9618WrwuCgKVLlyIxMRFhYWEYMmQITp48aXFObW0t5syZg7i4OERERGDMmDG4dOmSCz9F65jWoPia67fqUXC+HIDzpp5MvXke69keAzrHMtAhIiKr3Brs3Lp1Cz169MCaNWusvr5ixQqsXLkSa9aswdGjR6FWq5GRkYHKykrzOVlZWdi+fTu2bt2K/Px83Lx5E6NGjYLB4B3rXBoHAr6m8RQdp56IiMhdFIIgeMRm2gqFAtu3b8fjjz8OoCGrk5iYiKysLGRnZwNoyOIkJCRg+fLlmDZtGvR6Pdq1a4f3338f48ePBwBcvnwZSUlJ+PTTT/HII4+I+tkVFRVQqVTQ6/WIiopyyudz5KWdJ7HhwDm3/Gxn2TK1f7OuyGI37yQiInJE7PPbY1e9lpaWQqfTYfjw4eZjSqUSgwcPxsGDBwEABQUFqK+vtzgnMTERWq3WfI41tbW1qKiosPhyN7GLeL1F2/Bgq2twOPVERESu5rHBjk6nAwAkJCRYHE9ISDC/ptPpEBISgujoaJvnWJObmwuVSmX+SkpKknn00onZPsKb3Kiqx5q9p0U1CiQiInImjw12TBQKy8e/IAjNjjXl6JzFixdDr9ebvy5evCjLWFvDF9furNpzGg+8Km3vKyIiIrl5bLCjVjdM6zTN0Fy5csWc7VGr1airq0N5ebnNc6xRKpWIioqy+PIEmVoNfv9QsruHIStdhbS9r4iIiOTmscFOcnIy1Go18vLyzMfq6uqwb98+DBw4EADQu3dvBAcHW5xTVlaG4uJi8znewmAU8Pqe0/jLV6XuHopTiN37ioiISG5ubSp48+ZN/PDDD+bvS0tLUVRUhJiYGNx1113IyspCTk4OUlJSkJKSgpycHISHh2PSpEkAAJVKhSlTpmD+/PmIjY1FTEwMFixYgG7duiE9Pd1dH0uyXcVlWLrjpN1tI7xZ472vmlZnEREROZtbg51vvvkGDz/8sPn7efPmAQCefvppbNy4EQsXLkR1dTVmzpyJ8vJy9OvXD7t370ZkZKT5PatWrUJQUBDGjRuH6upqDBs2DBs3bkRgoPVdtl3NUan1ruIyzNhUaLWzsKdThQWhovq26LGL3RqDiIhITh7TZ8ednNVnZ1dxGZbtLEGZ/s5DXqMKxZLRqcjUamAwChi0fK/F696gbVgwXv1VNwCQFKhZ67tDRETUUl7fZ8fbmTI2TQMZnf7Ogt0jpde9LtABgDefbOh4bO6KHKV0+J62YcEwCgLX7RARkcsx2HECg1HAsp0lVjMepmPLdpZAp6925bBaTYGGzFT/TneyM5laDQ4sGobn0rvYfe+N6no8+devMWg5S9GJiMi1GOw4gaOMjWnB7vVbda4blAgKG//d+HtrG3YGBigwNz0Fb03u5XBT08aZLSIiIldgsOMEYhfiXrrhWZkdtSoUb03uhbdauGFnplaD/Oyh+PuUfmgbFmz1nMaZLU5pERGRK7i1GstXxUfaz26YfFR02ckjEe+FkV3xzAPJ5qxNRqq6RRt2BgYoEBCgwI3qepvnsBSdiIhcicGOE5j2udLpa2xWKkWFBnnUNFZcpNIimDFt2NkSYjNbLEUnIiJX4DSWEzTe58pWLqSi5rbrBiTCuatVsl1LbGZL7HlEREStwWDHScxl2Q4W7HqKrUcvyLaGxtEO7qaqrr7JMbL8PCIiInsY7DhRplaDfX94GDER1hfrehLTGho52Mts2avqIiIicgYGO05WcL4c12/ZXqzrSeRcQ2MrsyWmqouIiEhOXKDsZN60CFfuNTSZWk2Lq7qIiIjkwmDHybxhEa4CDRkXZ6yhaU1VFxERkRw4jeVkjhbruhvX0BARka9jsONkjRfreiLTGpqMVDUOnbmGj4p+xKEz19jdmIiIfAansVwgU6vB7x9Kxl++KnX3UAA0dEuOi1Sa19DklegwaPlei/28NKpQLBmdyoXERETk9ZjZcQGDUcCO456x8aVC0RDIPNazPQZ0jkVeiQ4zNhU227iUG3YSEZGvYLDjAo52QXclQQBmbT6GXcVlMBgFLNtZYnVLC27YSUREvoLBjgt4Yvn5sp0lOHz2mt0grPGGnURERN6KwY4LOLv8fOqDd0s63xTEHDpzTdT5nhisERERicVgxwWcWX4eERKIRY+m4q3JvaCRvA+XuOkpb+gVREREZAuDHScxGAVzKfeR0ut4YWRXp/ycW3UGHCm9jkytBvnZQ7Flan/MfvgeUe8d0CmOG3YSEZHPY+m5E+wqLsOynSXNSrl/92AyNuSXQu71vqZpJlO34r7JMfiw8BJ0+hqruRtTx+T+nWOxZHQqZmwqhAKWeR42GyQiIl/BzI7MdhWX2SzlXr9f/kAHaD7NJGXXcW7YSUREvo6ZHRmJKeWWk709rUxBTNMMk9pKs0Bu2ElERL6MwY6MXNlPR8w0k5Qghht2EhGRr2KwIyNXlmhby9BYwyCGiIj8HYMdGbmqRPuFkV3xzAPJnGYiIiISgQuUZeSon46plHvNhDQoWhGnxEUqGegQERGJxGBHRmKroGIjlRBasWKZTf6IiIjEY7AjMzGl3C1d28Mmf0RERNJxzY4TOKqCak1mhk3+iIiIpGGw4yT2qqBMa3tsdTi2JkABTH0wmU3+iIiIJOI0lhvYW9tjiyAAb39Vil3FZc4bGBERkQ9isOMmttb22JqhMmWAlu0sgcEZe04QERH5KE5juVHTtT1XK2vx0iff2TxfAFCmr8GR0utsFEhERCQSgx03MhgFi0XMRpEZG1d2aiYiIvJ2DHbcZFdxWbNNOmMiQkS9l312iIiIxGOw42IGo4A1e09j1Z7TzV4rv1Vn9732djknIiIi6xjsuNCu4jIs3VECXYX1aSh7k1hidjknIiKi5hjsuMiu4jLM2FQouq9OTEQwrt+qN38vdpdzIiIissRgxwUMRgHLdpaIDnQA4IVR90EdFWq1AzMRERGJx2DHBY6UXrdYiCyGOiqU5eVEREQy8JmmgmvXrkVycjJCQ0PRu3dv7N+/391DMpNSKs7NPomIiOTlE8HOP/7xD2RlZeH555/HsWPH8OCDD+LRRx/FhQsX3D00ANJLxbkImYiISD4+EeysXLkSU6ZMwe9+9zt07doVq1evRlJSEtatW+fuoQG4s/Gno/BFHaXEusm9uAiZiIhIRl4f7NTV1aGgoADDhw+3OD58+HAcPHjQ6ntqa2tRUVFh8eVMYjb+fC69Cw4sGsZAh4iISGZeH+xcvXoVBoMBCQkJFscTEhKg0+msvic3Nxcqlcr8lZSU5PRx2tr4U6MKxVuTe2FuegqnroiIiJzAZ6qxFArLQEEQhGbHTBYvXox58+aZv6+oqHBZwNN440+WlBMRETmf1wc7cXFxCAwMbJbFuXLlSrNsj4lSqYRSqXTF8JoJDFCwpJyIiMiFvH4aKyQkBL1790ZeXp7F8by8PAwcONBNoyIiIiJP4fWZHQCYN28ennrqKfTp0wcDBgzA22+/jQsXLmD69OnuHhoRERG5mU8EO+PHj8e1a9fw4osvoqysDFqtFp9++ik6duzo7qERERGRmykEQZCyZZNPqqiogEqlgl6vR1RUlLuHQ0RERCKIfX57/ZodIiIiInsY7BAREZFPY7BDREREPo3BDhEREfk0BjtERETk03yi9Ly1TAVpzt4QlIiIiORjem47KixnsAOgsrISAFyyPxYRERHJq7KyEiqVyubr7LMDwGg04vLly4iMjLTYPNS0QejFixfZf8cJeH+dj/fYuXh/nYv31/m8/R4LgoDKykokJiYiIMD2yhxmdgAEBASgQ4cONl+Pioryyv8ReAveX+fjPXYu3l/n4v11Pm++x/YyOiZcoExEREQ+jcEOERER+TQGO3YolUosWbIESqXS3UPxSby/zsd77Fy8v87F++t8/nKPuUCZiIiIfBozO0REROTTGOwQERGRT2OwQ0RERD6NwQ4RERH5NAY7NqxduxbJyckIDQ1F7969sX//fncPySvk5ubi/vvvR2RkJOLj4/H444/j1KlTFucIgoClS5ciMTERYWFhGDJkCE6ePGlxTm1tLebMmYO4uDhERERgzJgxuHTpkis/ilfIzc2FQqFAVlaW+Rjvb+v9+OOPmDx5MmJjYxEeHo6ePXuioKDA/Drvccvdvn0bf/rTn5CcnIywsDB06tQJL774IoxGo/kc3l9pvvrqK4wePRqJiYlQKBT417/+ZfG6XPezvLwcTz31FFQqFVQqFZ566incuHHDyZ9OJgI1s3XrViE4OFhYv369UFJSIsydO1eIiIgQzp8/7+6hebxHHnlEePfdd4Xi4mKhqKhIGDlypHDXXXcJN2/eNJ/z6quvCpGRkcKHH34onDhxQhg/fryg0WiEiooK8znTp08X2rdvL+Tl5QmFhYXCww8/LPTo0UO4ffu2Oz6WRzpy5Ihw9913C927dxfmzp1rPs772zrXr18XOnbsKDzzzDPC119/LZSWlgp79uwRfvjhB/M5vMct9/LLLwuxsbHCxx9/LJSWlgoffPCB0KZNG2H16tXmc3h/pfn000+F559/Xvjwww8FAML27dstXpfrfmZmZgparVY4ePCgcPDgQUGr1QqjRo1y1cdsFQY7VvTt21eYPn26xbF7771XWLRokZtG5L2uXLkiABD27dsnCIIgGI1GQa1WC6+++qr5nJqaGkGlUglvvfWWIAiCcOPGDSE4OFjYunWr+Zwff/xRCAgIEHbt2uXaD+ChKisrhZSUFCEvL08YPHiwOdjh/W297OxsYdCgQTZf5z1unZEjRwq//e1vLY6NHTtWmDx5siAIvL+t1TTYket+lpSUCACEw4cPm885dOiQAED4f//v/zn5U7Uep7GaqKurQ0FBAYYPH25xfPjw4Th48KCbRuW99Ho9ACAmJgYAUFpaCp1OZ3F/lUolBg8ebL6/BQUFqK+vtzgnMTERWq2Wv4OfzZo1CyNHjkR6errFcd7f1tuxYwf69OmDJ554AvHx8UhLS8P69evNr/Met86gQYPw+eef4/vvvwcAHD9+HPn5+RgxYgQA3l+5yXU/Dx06BJVKhX79+pnP6d+/P1QqlVfcc24E2sTVq1dhMBiQkJBgcTwhIQE6nc5No/JOgiBg3rx5GDRoELRaLQCY76G1+3v+/HnzOSEhIYiOjm52Dn8HwNatW1FYWIijR482e433t/XOnj2LdevWYd68efjjH/+II0eO4Nlnn4VSqcSvf/1r3uNWys7Ohl6vx7333ovAwEAYDAa88sormDhxIgD+b1huct1PnU6H+Pj4ZtePj4/3invOYMcGhUJh8b0gCM2OkX2zZ8/Gt99+i/z8/GavteT+8ncAXLx4EXPnzsXu3bsRGhpq8zze35YzGo3o06cPcnJyAABpaWk4efIk1q1bh1//+tfm83iPW+Yf//gHNm3ahM2bN+O+++5DUVERsrKykJiYiKefftp8Hu+vvOS4n9bO95Z7zmmsJuLi4hAYGNgsUr1y5UqzyJhsmzNnDnbs2IEvvvgCHTp0MB9Xq9UAYPf+qtVq1NXVoby83OY5/qqgoABXrlxB7969ERQUhKCgIOzbtw9vvPEGgoKCzPeH97flNBoNUlNTLY517doVFy5cAMD/DbfWH/7wByxatAgTJkxAt27d8NRTT+G5555Dbm4uAN5fucl1P9VqNf7zn/80u/5PP/3kFfecwU4TISEh6N27N/Ly8iyO5+XlYeDAgW4alfcQBAGzZ8/Gtm3bsHfvXiQnJ1u8npycDLVabXF/6+rqsG/fPvP97d27N4KDgy3OKSsrQ3Fxsd//DoYNG4YTJ06gqKjI/NWnTx88+eSTKCoqQqdOnXh/W+mBBx5o1i7h+++/R8eOHQHwf8OtVVVVhYAAy0dPYGCgufSc91dect3PAQMGQK/X48iRI+Zzvv76a+j1eu+45+5YFe3pTKXnGzZsEEpKSoSsrCwhIiJCOHfunLuH5vFmzJghqFQq4csvvxTKysrMX1VVVeZzXn31VUGlUgnbtm0TTpw4IUycONFqGWSHDh2EPXv2CIWFhcLQoUP9tqzUkcbVWILA+9taR44cEYKCgoRXXnlFOH36tPD3v/9dCA8PFzZt2mQ+h/e45Z5++mmhffv25tLzbdu2CXFxccLChQvN5/D+SlNZWSkcO3ZMOHbsmABAWLlypXDs2DFzuxS57mdmZqbQvXt34dChQ8KhQ4eEbt26sfTc27355ptCx44dhZCQEKFXr17m0mmyD4DVr3fffdd8jtFoFJYsWSKo1WpBqVQKDz30kHDixAmL61RXVwuzZ88WYmJihLCwMGHUqFHChQsXXPxpvEPTYIf3t/V27twpaLVaQalUCvfee6/w9ttvW7zOe9xyFRUVwty5c4W77rpLCA0NFTp16iQ8//zzQm1trfkc3l9pvvjiC6v/7j799NOCIMh3P69duyY8+eSTQmRkpBAZGSk8+eSTQnl5uYs+ZesoBEEQ3JNTIiIiInI+rtkhIiIin8Zgh4iIiHwagx0iIiLyaQx2iIiIyKcx2CEiIiKfxmCHiIiIfBqDHSIiIvJpDHaIiIjIpzHYISK/o1Ao8K9//cvdwyAiF2GwQ0ROc/DgQQQGBiIzM1Pye++++26sXr1a/kGJ8Mwzz0ChUEChUCAoKAh33XUXZsyYgfLycnz55Zfm12x9bdy4sdl57dq1w6OPPorjx4+75TMR+TMGO0TkNO+88w7mzJmD/Px8XLhwwd3DkSQzMxNlZWU4d+4c/vrXv2Lnzp2YOXMmBg4ciLKyMvPXuHHjzOeavsaPH2++zqlTp1BWVoZPPvkE5eXlyMzMhF6vd+MnI/I/DHaIyClu3bqFf/7zn5gxYwZGjRqFjRs3Njtnx44d6NOnD0JDQxEXF4exY8cCAIYMGYLz58/jueeeM2dGAGDp0qXo2bOnxTVWr16Nu+++2/z90aNHkZGRgbi4OKhUKgwePBiFhYWSx69UKqFWq9GhQwcMHz4c48ePx+7duxESEgK1Wm3+CgsLM5/b+JhJfHw81Go1+vbti9deew06nQ6HDx+WPB4iajkGO0TkFP/4xz/wi1/8Ar/4xS8wefJkvPvuu2i87/Ann3yCsWPHYuTIkTh27Bg+//xz9OnTBwCwbds2dOjQAS+++KI5WyJWZWUlnn76aezfvx+HDx9GSkoKRowYgcrKyhZ/lrNnz2LXrl0IDg5u8TUAmIOg+vr6Vl2HiKQJcvcAiMg3bdiwAZMnTwbQMCV08+ZNfP7550hPTwcAvPLKK5gwYQKWLVtmfk+PHj0AADExMQgMDERkZCTUarWknzt06FCL7//yl78gOjoa+/btw6hRo0Rf5+OPP0abNm1gMBhQU1MDAFi5cqWksTR27do1LFu2DJGRkejbt2+Lr0NE0jGzQ0SyO3XqFI4cOYIJEyYAAIKCgjB+/Hi888475nOKioowbNgw2X/2lStXMH36dHTp0gUqlQoqlQo3b96UvGbo4YcfRlFREb7++mvMmTMHjzzyCObMmSN5PB06dECbNm0QFxeH7777Dh988AHi4+MlX4eIWo6ZHSKS3YYNG3D79m20b9/efEwQBAQHB6O8vBzR0dEW61rECggIsJgKA5pPCT3zzDP46aefsHr1anTs2BFKpRIDBgxAXV2dpJ8VERGBe+65BwDwxhtv4OGHH8ayZcvw0ksvSbrO/v37ERUVhXbt2iEqKkrSe4lIHszsEJGsbt++jffeew+vvfYaioqKzF/Hjx9Hx44d8fe//x0A0L17d3z++ec2rxMSEgKDwWBxrF27dtDpdBYBT1FRkcU5+/fvx7PPPosRI0bgvvvug1KpxNWrV1v9uZYsWYI///nPuHz5sqT3JScno3Pnzgx0iNyIwQ4Ryerjjz9GeXk5pkyZAq1Wa/H1X//1X9iwYQOAhuBhy5YtWLJkCb777jucOHECK1asMF/n7rvvxldffYUff/zRHKwMGTIEP/30E1asWIEzZ87gzTffxGeffWbx8++55x68//77+O677/D111/jySefbFEWqakhQ4bgvvvuQ05OTquvRUSuxWCHiGS1YcMGpKenQ6VSNXvtV7/6FYqKilBYWIghQ4bggw8+wI4dO9CzZ08MHToUX3/9tfncF198EefOnUPnzp3Rrl07AEDXrl2xdu1avPnmm+jRoweOHDmCBQsWWPyMd955B+Xl5UhLS8NTTz2FZ599VrY1MvPmzcP69etx8eJFWa5HRK6hEJpOgBMRERH5EGZ2iIiIyKcx2CEiIiKfxmCHiIiIfBqDHSIiIvJpDHaIiIjIpzHYISIiIp/GYIeIiIh8GoMdIiIi8mkMdoiIiMinMdghIiIin8Zgh4iIiHza/weZte0uOpSHQQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      freq_Orange  freq_Apple  freq_Grape  freq_Strawberry  freq_Mango  \\\n",
      "7200     0.075811    0.084947    0.121932         0.080876    0.110240   \n",
      "9861     0.123081    0.048750    0.024607         0.117306    0.081603   \n",
      "623      0.027751    0.061387    0.081592         0.127617    0.138601   \n",
      "9401     0.036912    0.043561    0.058402         0.150794    0.094625   \n",
      "9782     0.129698    0.139246    0.094982         0.035717    0.109751   \n",
      "3529     0.005928    0.014527    0.119349         0.126688    0.132652   \n",
      "5386     0.112079    0.024854    0.066368         0.084173    0.139695   \n",
      "4350     0.100416    0.145916    0.080385         0.042994    0.068989   \n",
      "4347     0.172809    0.106432    0.098863         0.153964    0.026422   \n",
      "2486     0.090883    0.127489    0.082528         0.018125    0.094178   \n",
      "\n",
      "      freq_Banana  freq_Cherry  freq_Lemon  freq_Seven  freq_Bonus  ...  \\\n",
      "7200     0.097879     0.109519    0.122841    0.003884    0.061846  ...   \n",
      "9861     0.088373     0.027643    0.072375    0.007310    0.146570  ...   \n",
      "623      0.095361     0.053032    0.104478    0.007484    0.082368  ...   \n",
      "9401     0.087817     0.059932    0.139265    0.006016    0.178148  ...   \n",
      "9782     0.142596     0.090008    0.123846    0.006930    0.081476  ...   \n",
      "3529     0.133234     0.080751    0.122378    0.000190    0.076799  ...   \n",
      "5386     0.081099     0.065752    0.117082    0.005311    0.102740  ...   \n",
      "4350     0.136822     0.111651    0.099091    0.000916    0.107472  ...   \n",
      "4347     0.073439     0.130954    0.013716    0.005259    0.055762  ...   \n",
      "2486     0.141508     0.095820    0.050345    0.006768    0.121732  ...   \n",
      "\n",
      "      freq_Wild  multi_Orange  multi_Apple  multi_Grape  multi_Strawberry  \\\n",
      "7200   0.066089          1.74         0.38         1.29              0.32   \n",
      "9861   0.134220          0.71         0.74         1.45              1.33   \n",
      "623    0.083936          1.26         0.25         1.13              0.58   \n",
      "9401   0.115654          0.32         1.31         0.43              1.12   \n",
      "9782   0.045494          0.30         0.62         0.73              1.85   \n",
      "3529   0.107816          0.71         0.73         0.89              0.42   \n",
      "5386   0.112633          1.03         1.26         1.11              1.44   \n",
      "4350   0.049005          1.02         0.61         1.60              1.17   \n",
      "4347   0.131066          0.51         1.48         0.70              0.54   \n",
      "2486   0.048586          1.50         0.84         0.62              1.35   \n",
      "\n",
      "      multi_Mango  multi_Banana  multi_Cherry  multi_Lemon  multi_Seven  \n",
      "7200         0.42          1.52          1.30         1.31           13  \n",
      "9861         1.49          0.55          0.29         0.71            9  \n",
      "623          0.67          0.92          1.65         0.83           12  \n",
      "9401         1.05          0.41          0.29         0.65           13  \n",
      "9782         1.54          0.71          1.28         0.91           14  \n",
      "3529         0.94          0.28          0.94         1.18           13  \n",
      "5386         0.90          1.79          0.61         1.38            7  \n",
      "4350         1.34          0.36          1.69         1.58            7  \n",
      "4347         0.48          1.12          0.39         1.32            9  \n",
      "2486         1.24          0.64          1.60         0.65            8  \n",
      "\n",
      "[10 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korisnik/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "### Provera da li NN radi\n",
    "\n",
    "# NNM\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluation\n",
    "evaluation = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Loss:\", evaluation)\n",
    "# Prediction\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "# Visualize the model's performance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_test['RTP'], predictions[:, 0])\n",
    "plt.xlabel('Actual RTP')\n",
    "plt.ylabel('Predicted RTP')\n",
    "plt.show()\n",
    "\n",
    "model.save('trained_model.h5')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "filtered_predictions = predictions[(predictions[:, 0] >= 90) & (predictions[:,0] <= 95)]\n",
    "filtered_input_params = X_test[(predictions[:, 0] >= 90) & (predictions[:,0] <= 95)]\n",
    "print(filtered_input_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for optimization\n",
    "# hyperparameter ranges\n",
    "\n",
    "params = [\n",
    "    [32], # size\n",
    "    [2,3], # number of hidden layers\n",
    "    ['relu'], # activation function\n",
    "    [0.01,0.1], #learning rates\n",
    "    [100,120] # epochos\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fitness(individual):\n",
    "    print(individual)\n",
    "    # hypterparameters\n",
    "    input_size  = X_train_scaled.shape[1]\n",
    "    output_size = y_train.shape[1]\n",
    "\n",
    "    hidden_layer_size = (individual[0])\n",
    "    number_of_layers = (individual[1])\n",
    "    individual_activation = individual[2]\n",
    "    individual_learning_rate = individual[3]\n",
    "    individual_epochs = individual[4]\n",
    "\n",
    "    model = Sequential()\n",
    "    #first layer\n",
    "    model.add(Dense(units=hidden_layer_size,\n",
    "                    activation = individual_activation,\n",
    "                    input_shape =(input_size,)))\n",
    "    #hidden layers\n",
    "    for i in range(number_of_layers):\n",
    "        model.add(Dense(units = hidden_layer_size,\n",
    "                        activation = individual_activation\n",
    "                        )) \n",
    "    #output layer\n",
    "    model.add(Dense(output_size))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Adam(learning_rate = individual_learning_rate),\n",
    "        loss = 'mean_absolute_error' \n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train,epochs = individual_epochs,batch_size = 32,verbose = 0)\n",
    "\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    mae = mean_absolute_error(y_val,y_pred)\n",
    "\n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korisnik/.local/lib/python3.11/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/korisnik/.local/lib/python3.11/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is best fitness now  inf\n",
      "Ovo su fits\n",
      "<map object at 0x7f127c4036a0>\n",
      "[32, 2, 'relu', 0.01, 120]\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      " fitness   19.626730569007655\n",
      "[32, 3, 'relu', 0.01, 120]\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      " fitness   20.038349183579363\n",
      "[32, 2, 'relu', 0.1, 100]\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      " fitness   31.965258768361757\n",
      "[32, 2, 'relu', 0.1, 100]\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      " fitness   32.368368859506916\n",
      "[32, 3, 'relu', 0.1, 120]\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      " fitness   32.07511800836279\n",
      "[32, 3, 'relu', 1, 1]\n",
      "19/19 [==============================] - 0s 1ms/step\n",
      " fitness   74.51667938993586\n",
      "[32, 3, 4, 0.1, 100]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not interpret activation function identifier: 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb Cell 13\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb#X15sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOvo su fits\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb#X15sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(fits)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb#X15sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m child, fitness \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(offspring,fits):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb#X15sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     child\u001b[39m.\u001b[39mfitness\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m (fitness,)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m fitness  \u001b[39m\u001b[39m\"\u001b[39m, fitness)\n",
      "\u001b[1;32m/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#first layer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(units\u001b[39m=\u001b[39;49mhidden_layer_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                 activation \u001b[39m=\u001b[39;49m individual_activation,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                 input_shape \u001b[39m=\u001b[39;49m(input_size,)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#hidden layers\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/korisnik/Desktop/Projects/Slot_Machine_RI/network_optimization.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(number_of_layers):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/dtensor/utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[0;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[39mif\u001b[39;00m layout:\n\u001b[1;32m     94\u001b[0m             layout_args[variable_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_layout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m layout\n\u001b[0;32m---> 96\u001b[0m init_method(layer_instance, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m \u001b[39m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m layout_param_name, layout \u001b[39min\u001b[39;00m layout_args\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/layers/core/dense.py:125\u001b[0m, in \u001b[0;36mDense.__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mReceived an invalid value for `units`, expected \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ma positive integer. Received: units=\u001b[39m\u001b[39m{\u001b[39;00munits\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation \u001b[39m=\u001b[39m activations\u001b[39m.\u001b[39;49mget(activation)\n\u001b[1;32m    126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_bias \u001b[39m=\u001b[39m use_bias\n\u001b[1;32m    127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_initializer \u001b[39m=\u001b[39m initializers\u001b[39m.\u001b[39mget(kernel_initializer)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/activations.py:708\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[39melif\u001b[39;00m callable(identifier):\n\u001b[1;32m    707\u001b[0m     \u001b[39mreturn\u001b[39;00m identifier\n\u001b[0;32m--> 708\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    709\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not interpret activation function identifier: \u001b[39m\u001b[39m{\u001b[39;00midentifier\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    710\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not interpret activation function identifier: 4"
     ]
    }
   ],
   "source": [
    "# start\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-11.0,))\n",
    "creator.create(\"Individual\",list,fitness =  creator.FitnessMin)\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "#initialization\n",
    "toolbox.register(\"hidden_layers_size\", random.choice, params[0])\n",
    "toolbox.register(\"num_hidden_layers\", random.choice, params[1])\n",
    "toolbox.register(\"activation_function\", random.choice, params[2])\n",
    "toolbox.register(\"learning_rate\", random.choice, params[3])\n",
    "#toolbox.register(\"loss_function\", random.choice, params[4])\n",
    "toolbox.register('epochs',random.choice,params[4])\n",
    "\n",
    "def create_individual():\n",
    " return [toolbox.hidden_layers_size(), toolbox.num_hidden_layers(), toolbox.activation_function(), toolbox.learning_rate(),\n",
    "            toolbox.epochs()]\n",
    "\n",
    "\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual,create_individual)\n",
    "toolbox.register(\"population\",tools.initRepeat,list, toolbox.individual)\n",
    "\n",
    "\n",
    "toolbox.register(\"evaluate\", fitness)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutUniformInt, low=0, up=len(create_individual()) - 1, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "## generic algorithm\n",
    "best_fitness = float('inf')\n",
    "\n",
    "population_size = 50\n",
    "num_generation = 10\n",
    "cxpb = 0.5\n",
    "mutpb = 0.1\n",
    "population = toolbox.population(n = population_size)\n",
    "\n",
    "\n",
    "for g in range(num_generation):\n",
    "    print(\"This is best fitness now \", best_fitness)\n",
    "    offspring  = algorithms.varAnd(population,toolbox,cxpb=0.5,mutpb=0.4)\n",
    "    fits  = map(toolbox.evaluate,offspring)\n",
    "    print(\"Ovo su fits\")\n",
    "    for child, fitness in zip(offspring,fits):\n",
    "        child.fitness.values = (fitness,)\n",
    "        print(\" fitness  \", fitness)\n",
    "    \n",
    "    best_individual = tools.selBest(population, k=1)[0]\n",
    "    best_fitness_current = best_individual.fitness.values[0]\n",
    "\n",
    "    print(\"This is best fitness now \", best_fitness_current)\n",
    "    population[:] = offspring\n",
    "    if best_fitness_current < best_fitness:\n",
    "        best_fitness = best_fitness_current\n",
    "    else:\n",
    "        if g >= 4:\n",
    "            break\n",
    "\n",
    "# Get the best individual from the final population\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "best_hyperparameters = best_individual\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_network (size_units,num_of_layers,activation_f,learning_rates,num_epo):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation=activation_f, input_shape=(X_train_scaled.shape[1],)))\n",
    "    for _ in range(num_of_layers):\n",
    "        model.add(Dense(64, activation=activation_f))\n",
    "    model.add(Dense(5))\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "    model.fit(X_train_scaled,y_train,epochs = num_epo,batch_size = 32,verbose = 1)\n",
    "\n",
    "    evaluation = model.evaluate(X_test_scaled,y_test)\n",
    "\n",
    "    print(\"Loss: \", evaluation)\n",
    "\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "\n",
    "    mae = mean_squared_error(y_test,predictions)\n",
    "\n",
    "    loaded_model.load_weights('trained_model.h5')\n",
    "    scaler = joblib.load('scaler.pkl')\n",
    "    print(\"MAE: \",mae)\n",
    "\n",
    "    model.save('trained_model.h5')\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "    # Load the trained model and scaler\n",
    "    loaded_model = Sequential()\n",
    "    loaded_model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "    loaded_model.add(Dense(64, activation='relu'))\n",
    "    loaded_model.add(Dense(5))\n",
    "    loaded_model.load_weights('trained_model.h5')\n",
    "    scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "    # Generate a set of input parameters for RTP prediction\n",
    "    input_params = pd.DataFrame({\n",
    "        'freq_Orange': [0.1],\n",
    "        'freq_Apple': [0.2],\n",
    "        'freq_Grape': [0.3],\n",
    "        'freq_Strawberry': [0.1],\n",
    "        'freq_Mango': [0.1],\n",
    "        'freq_Banana': [0.05],\n",
    "        'freq_Cherry': [0.05],\n",
    "        'freq_Lemon': [0.05],\n",
    "        'freq_Seven': [0.05],\n",
    "        'freq_Bonus': [0.1],\n",
    "        'multi_Orange': [1.2],\n",
    "        'multi_Apple': [1.5],\n",
    "        'multi_Grape': [1.3],\n",
    "        'multi_Strawberry': [1.4],\n",
    "        'multi_Mango': [1.6],\n",
    "        'multi_Banana': [1.3],\n",
    "        'multi_Cherry': [1.4],\n",
    "        'multi_Lemon': [1.2],\n",
    "        'multi_Seven': [1.6]\n",
    "    })\n",
    "\n",
    "    # Scale the input parameters\n",
    "    input_params_scaled = scaler.transform(input_params)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = loaded_model.predict(input_params_scaled)\n",
    "\n",
    "    # Filter the predictions based on RTP threshold (e.g., 90%)\n",
    "    filtered_predictions = predictions[predictions[:, 0] >= 90]\n",
    "\n",
    "    # Get the corresponding input parameters for the filtered predictions\n",
    "    filtered_input_params = input_params.loc[predictions[:, 0] >= 90]\n",
    "\n",
    "    # Print the input parameters for RTP 90%\n",
    "    print(filtered_input_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
